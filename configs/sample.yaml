model_name: meta-llama/Llama-3.1-70B-Instruct
gpus: 2
input_path: data/B2_openai.jsonl
output_path: output/bush-multi/
hf_token: HF_TOKEN_GOES_HERE
quantization: True
sampling_params:
  temperature: [0.2, 0.4, 0.6, 0.8, 1.0]
  n: 3
  max_new_tokens: 2048